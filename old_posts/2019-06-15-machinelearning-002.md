---
title: 【西瓜书】 002 模型评估与选择
date: 2019-06-15 16:26:00
---

### 经验误差与过拟合

错误率（error rate）：分类错误的样本数占总样本总数的比例

误差（error）：学习器的实际预测输出与样本的真实输出之间的差异

训练误差（train error）/经验误差（empirical error）：学习器在训练集上的误差

泛化误差（generalization error）：学习器在新样本上的误差


只能使经验误差最小化从而得到泛化误差小的学习器。


当学习器把训练样本学得太好的话，和有可能把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，导致泛化性能下降。
这种现象在机器学习中被称为过拟合。
与过拟合相对的是欠拟合，这是指对训练样本的一般性质尚未学习好。



导致过拟合的因素有很多，最常见的情况是由于学习能力过于强大，以至于把训练样本所包含的不太一样的特性都学习到了，而欠拟合则通常是由于学习能力地下而导致的。

欠拟合比较容易克服，增加学习能力（拓展决策树的分支，增加神经网络训练轮数）。

各类算法都必然带有一些针对过拟合的措施；
然而必须意识到，过拟合是无法彻底避免的，我们做能做到的只有缓解。

机器学习面临的问题通常是NP甚至更难，
而有效的学习算法是在多项式时间内运行完成，
若可彻底避免过拟合，
则通过经验误差最小化就能获得最优解，
这就意味着我们构造性地证明了P=NP；
因此，只要相信P!=NP过拟合就不可避免。


### 评估方法

通常需要使用一个测试机来测试学习其对新样本的判别能力，
然后以测试误差作为泛化误差的近似。
通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。
测试集应该近可能与训练集互斥。


常见训练集与测试集处理方法
1. 留出法
将数据集划分为两个互斥的集合，作为训练集和测试集。
划分要尽可能保持数据分析的一致性。
（分类问题要保持样本的类别相似性）
即使在给定训练/测试集的样本比例后，让存在多种划分方式对初始数据集进行分割。
单次使用留出法得到的结果往往不够稳定，在使用留出法时，一般要采取若干次随即划分、重复进行实验评估后取平均值作为留出法的评估结果。
若令训练集包含绝大多数样本，则训练出的模型更接近于数据集，由于测试集比较小，评估结果不够稳定准确；
若测试机包含更多样本，则训练集和测试集差别更大了，评估的模型与用数据集训练出的模型相比可能有较大的差别，从而降低了评估模型的保真性。
这个问题没有完美的解决方案，常见做法是将2/3~4/5的样本用于训练，剩余样本用于测试。
2. 交叉验证法

3. 自助法
4. 调参与最终模型


### 性能度量（performance measure）

在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果；
这意味着模型的好坏是相对的，不仅取决于算法和数据还取决于任务需求。

回归任务最常用的性能度量是均方误差（mean squared error）：
$$E(f;D) = \frac{1}{m} \sum_{i=1}^{m}{(f(x_i) - y_i)^2}$$

更一般的，对于数据分布D和概率密度函数p(.)，均方误差可描述为：
$$E(f;D) = \int_{x~D}{(f(x) - y)^2 P(x) dx}$$

#### 错误率与精度

错误率和精度是分类任务常用的两种性能度量，
即适用于二分类任务，也适用于多分类任务。

错误率是分类错误的样本数占样本总数的比例。

精度是分类正确的样本数占样本总数的比例。
![ml-02-1](/images/machine-learning/ml2.1.jpeg)


#### 查准率、查全率与F1

对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例（true positive）、假正例（false positive）、真反例（true negative）、假反例（false negative）四种情形。

TP + FP + TN + FN = 样例总数

分类结果的“混淆矩阵”（confusion matrix）：
![ml-02-2](/images/machine-learning/ml2.2.jpg)


查准率P和查全率R分别定义为：

$$P = \frac{TP}{TP + FP}$$

预测的正例中，正确被预测的正例占的比例。

$$R = \frac{TP}{TP + FN}$$

真实正例中，正确被预测的正例占的比例。



查准率和查全率是一对矛盾的度量。
查准率高时，查全率往往偏低；查全率高时，查准率往往偏低。

根据学习器的预测结果对样例进行排序，
排在前面的是学习器认为“最可能”是正例的样本，
排在最后的则是学习器认为“最不可能”是正例的样本。
按此顺序逐个把样本作为正例进行预测，
则每次可以计算出当前的查全率、查准率。

以查准率为纵轴、查全率为横轴作图就得到查准率-查全率曲线，简称为“P-R曲线”，
显示该曲线的图就是“P-R图”：
![ml-02-3](/images/machine-learning/ml2.3.jpg)

若一个学习器的曲线被另一个学习器的曲线完全包住，
则可断言后者性能优于前者。

若两个学习器的曲线发生交叉，
这时一个比较合理的判据就是比较曲线下面积的大小。
但这个值不太容易估算。
这样的度量有：平衡点、F1度量


平衡点（Break-Event Point，简称BEP）是查准率=查全率时的取值。

F1度量：
$$F1 = \frac{2 * P * R}{P + R} = \frac{2 * TP}{样例总数 + TP - TN}$$

一些应用中，对查准率和查全率的重视程度有所不同。
F1度量的一般形式$F_{\beta}$，能表达出对查准率/查全率的不同偏好
$$F_{\beta} = \frac{(1 + \beta^2) * P * R}{(\beta^2 * P) + R}$$

其中$\beta > 0$度量了查全率对查准率的相对重要性。
$\beta = 1$时，退化为标准的F1；
$\beta > 1$时，查全率有更大影响；
$\beta < 1$时，查准率有更大影响。

在n个二分类混淆矩阵综合考虑查准率和查全率：

一种直接的做法是先在各混淆矩阵上分别计算出查准率和查全率，再计算平均值。
这样得到“宏查准率”（macro-P）、“宏查全率”（macro-R）、“宏F1”（macro-F1）
$$macro-P = \frac{1}{n}\sum_{i=1}^{n}{P_i}$$
$$macro-R = \frac{1}{n}\sum_{i=1}^{n}{R_i}$$
$$macro-F1 = \frac{2 * macro-P * macro-R}{macro-P + macro-R}$$

另一种做法是将各个混淆矩阵的对应元素进行平均，
得到$\bar{TP}$、$\bar{FP}$、$\bar{TN}$、$\bar{FN}$，
在基于这些平均值计算出“微查准率”（micro-P）、“微查全率”（micro-R）、“微F1”（micro-F1）：
$$micro-P = \frac{\bar{TP}}{\bar{TP} + \bar{FP}}$$
$$micro-P = \frac{\bar{TP}}{\bar{TP} + \bar{FN}}$$
$$micro-F1 = \frac{2 * micro-P * micro-R}{micro-P + micro-R}$$


#### ROC与AUC

很多学习器是为测试样本产生一个实值或概率预测，
然后将这个预测值与一个分类阈值进行比较，
若大于阈值则分为正类；否则为反类。

分类过程就相当于在这个可能性排序中以某个截断点（cut point）将样本分为两个部分。

在不同的应用任务中，根据任务需求的不同采用不同的截断点。

排序本身质量的好坏，体现了综合考虑学习器在不同任务下的“期望泛化性能”的好坏。


ROC（Receiver Operating Characteristic）曲线根据学习器的预测结果对样例进行排序，
按此顺序诸葛不样本作为正例进行预测，
每次计算出真正例率（True Position Rate，TPR）和假正例率（False Position Rate，FPR）作为纵轴和横轴。
$$TPR = \frac{TP}{TP + FN}$$
$$FPR = \frac{FP}{TN + FP}$$

![ml-02-4](/images/machine-learning/ml2.4.jpg)

与P-R图相似，若有一个学习器的ROC曲线被另一个学习器曲线完全包住，
则可断言后者性能优于前者；
若两个学习器的ROC曲线交叉，
较为合理的判据是比较ROC曲线下的面积，即AUC（Area Under ROC Curve）。

AUC可通过对ROC曲线下各部分面积求和得到，
假定ROC曲线是由坐标为${(x_1, y_1), (x_2, y_2), ...(x_m, y_m)}$的点按序连接而形成$(x_1 = 0, x_m = 1)$，则AUC可估算为：
$$AUC = \frac{1}{2}\sum_{i=1}^{m-1}{(x_{i+1} - x_i) * (y_i + y_{i+1})}$$

AUC考虑的是样本预测的排序质量，
因此它与排序误差有紧密联系。
给定$m^+$个正例和$m^-$个反例，令$D^+$和$D^-$分别表示正、反例集合，
则排序“损失”（loss）定义为：
$$\ell_{rank} = \frac{1}{m^+ * m^-} \sum_{x^+ \in D^+}{\sum_{x^- \in D^-}{(\amalg(f(x^+) < f(x^-)) + \frac{1}{2} \amalg(f(x^+) = f(x^-)))}}$$

即考虑每一对正、反例，
若正例的预测值小于反例，则记一个“罚分”，
若相等，则记0.5个“罚分”。

$\ell_{rank}$对应的是ROC曲线之上的面积：
若一个正例在ROC曲线上对应标记点的坐标为$(x, y)$，
则x恰是排序在其之前的反例所占的比例，即假正例率，因此有

$$AUC = 1 - \ell_{rank}$$



#### 代价敏感错误率与代价曲线

为权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”(unequal cost)。

![ml-02-6](/images/machine-learning/ml2.6.png)

在非均等代价下，ROC曲线不能直接反映出学习器的期望总代价，
而代价函数（cost curve）则可达到该目的。

代价函数曲线图的横轴是取值为[0, 1]的正例概率代价
$$P_{cost} = \frac{P * cost_{01}}{p * cost_{01} + (1 - p) * cost_{10}}$$

其中p是样例为正例的概率；
纵轴是取值为[0, 1]的归一化代价

$$cost_{norm} = \frac{FNR * p * cost_{01} + FPR * (1 - p) * cost_{10}}{p * cost_{01} + (1 - p) * cost_{10}}$$

FPR是假正例率，FNR是假反例率。


代价曲线的绘制：

ROC曲线上每一点都对应了代价平面上的一条线段，
设ROC曲线上点的坐标为(TPR, FPR)，
则可计算出FNR，
然后再代价平面上绘制一条从(0, FPR)到(1, FNR)的线段，
线段下的面积即表示了该条件下的期望的总体代价；
如此将ROC去线上的每个点转化为代价平面上的一条线段，
然后取所有线段的下界，
围成的面积即为在所有条件下学习器的期望总体代价。

![ml-02-7](/images/machine-learning/ml2.7.jpg)


### 比较检验

计算机性能评估涉及的重要因素
1. 希望比较的是泛化性能，然而通过实验评估获得的是测试集上的性能，两者的对比结果未必相同
2. 测试集上的性能与测试机本身的选择有很大关系
3. 机器学习算法本身就具有一定的随机性


统计假设检验为我们进行学习器的比较提供了重要依据。

1. 假设检验
2. 交叉验证t检验
3. McNemar检验
4. Firedman检验与Nemenyi后续检验


### 偏差与方差

偏差方差分解是解决学习算法泛化性能的一种重要工具。

学习算法的期望预测：
$$\bar{f}(x) = E_D[f(x; D)]$$

使用样本数相同的不同训练集产生的方差为：
$$var(x) = E_D[(f(x; D) - \bar{f}(x))^2]$$

噪声为
$$\varepsilon^2 = E_D[(y_D - y)^2]$$

期望输出与真是标记的差别为偏差：
$$bias^2(x) = (\bar{f}(x) - y)^2$$


泛化误差可分解为偏差、方差与噪声之和。

偏差度量了学习算法的期望预测与真实结果的偏离程度，
即刻画了学习算法本身的拟合能力。

方差度量了同样大小的训练集的变动所导致的学习性能的变化，
即刻画了数据扰动所造成的影响。

噪声则表达了在当前的任务上任何算法所能达到的期望泛化误差下界，
即刻画了学习问题本身的难度。

偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度决定的。
给定学习任务，为取得好的泛化性能，则需是变差较小，既能充分拟合数据，并且使方差较少，即使的数据扰动的影响小。

偏差和误差是有冲突的，被称为偏差-方差窘境
