---
title: 【NLP】【CS224N】transformer
date: 2018-12-07 11:48:31
---
FaceBook的《Convolutional Sequence to Sequence Learning》
和Google的《Attention is All You Need》，
它们都算是Seq2Seq上的创新，本质上来说，都是抛弃了RNN结构来做Seq2Seq任务。



深度学习做NLP的方法，基本上都是先将句子分词，然后每个词转化为对应的词向量序列。
这样一来，每个句子都对应的是一个矩阵X=(x1,x2,…,xt)，
其中xi都代表着第i个词的词向量（行向量），维度为d维，故X∈ℝn×d。
这样的话，问题就变成了编码这些序列了。

第一个基本的思路是RNN层，RNN的方案很简单，递归式进行：yt=f(yt−1,xt)

不管是已经被广泛使用的LSTM、GRU还是最近的SRU，都并未脱离这个递归框架。
RNN结构本身比较简单，也很适合序列建模，但RNN的明显缺点之一就是无法并行，因此速度较慢，
这是递归的天然缺陷。
另外我个人觉得RNN无法很好地学习到全局的结构信息，因为它本质是一个马尔科夫决策过程。(???)

RNN与HMM基本结构上是挺像的，都是通过hidden state 的演化来刻画 序列间的依赖关系。
不同是：
1. 隐状态的表示: hmm是onehot, RNN是分布表示，
RNN的表示能力强很多，或者说在面对高维度时，表示效率更高。
类似nlp里，对单个token的表示，一种是onehot, 一种是word vector 。
2. 隐状态的演化方式: hmm是线性的，RNN是高度非线性。
3. 在垂直方向上，实际中的lstm还会增加depth, 来增加不同层面的抽象表示，
也会使得表示能力指数增加，随着depth 增加。

RNN也是基于马尔可夫假设的，当前的隐状态仅依赖前一个时刻的隐状态。
在有马尔可夫假设的模型中，不代表距离超过1的两个状态是无依赖的。

印象中有paper证明， 在HMM中两个状态的依赖关系随距离指数衰减，
而在RNN中是power law decay.  
也就是大家通常说的 rnn可以略好的刻画 long term dependency.

另，SLAM中还有种算法跟hmm类似，kalman filter.

第二个思路是CNN层，其实CNN的方案也是很自然的，窗口式遍历，比如尺寸为3的卷积，就是
yt=f(xt−1,xt,xt+1)

在FaceBook的论文中，纯粹使用卷积也完成了Seq2Seq的学习，
是卷积的一个精致且极致的使用案例，热衷卷积的读者必须得好好读读这篇文论。
CNN方便并行，而且容易捕捉到一些全局的结构信息，笔者本身是比较偏爱CNN的，
在目前的工作或竞赛模型中，我都已经尽量用CNN来代替已有的RNN模型了，
并形成了自己的一套使用经验。

Google提供了第三个思路：纯Attention！单靠注意力就可以！RNN要逐步递归才能获得全局信息，
因此一般要双向RNN才比较好；
CNN事实上只能获取局部信息，是通过层叠来增大感受野；
Attention的思路最为粗暴，它一步到位获取了全局信息！
它的解决方案是：
yt=f(xt,A,B)

其中A,B是另外一个序列（矩阵）。
如果都取A=B=X，那么就称为Self Attention，
它的意思是直接将xt与原来的每个词进行比较，最后算出yt！


Google的一般化Attention思路也是一个编码序列的方案，
因此我们也可以认为它跟RNN、CNN一样，都是一个序列编码的层。

前面给出的是一般化的框架形式的描述，事实上Google给出的方案是很具体的。
首先，它先把Attention的定义给了出来：
$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$






---
参考资料：  
1. [Transformer](https://blog.csdn.net/YQMind/article/details/80864133)
2. [illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)
3. [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
4. [《Attention is All You Need》浅读（简介+代码）](https://kexue.fm/archives/4765)
5. [对Attention is all you need 的理解](https://blog.csdn.net/mijiaoxiaosan/article/details/73251443)
6. [一文读懂「Attention is All You Need 附代码实现](https://yq.aliyun.com/articles/342508?utm_content=m_39938)
7. [作者：a88i99 来源：知乎](https://www.zhihu.com/question/57396443/answer/263019702)
