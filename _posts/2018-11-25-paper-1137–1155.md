---
title: 【论文笔记】A Neural Probabilistic Language Model
date: 2018-11-25 08:10:31
---

统计语言模型的目标之一就是学习语言中单词序列的联合概率函数。  
由于维度灾难(curse of dimensionality)，这件事非常困难：测试模型所用的单词序列可能与训练过程中看到的所有单词序列不同。  

传统方法基于[n-grams](https://zhuanlan.zhihu.com/p/32829048)通过连接测试集中非常短的重叠序列获得通用性。  
该论文提出了学习单词分布式表示(learning a distributed representation for words)来克服维度灾难。  
这种表示使得每一个训练集中的句子都能告诉模型语义相邻句子的指数数量。  

该模型学习内容：    
(1)每个词的分布式表示  
(2)以这些表示形式表示的词序列的概率函数  

之所以能有很好的泛化性是因为一组以前从未见过的单词序列与一个已经见过的句子的构成单词相似(在某种意义上说，具有相近的表示形式)，那么这两个序列就具有很高的概率相似。  
在合理的时间内训练如此大的模型(包含数百万个参数)本身就是一个巨大的挑战。通过使用神经网络进行概率函数实验，在两个文本语料库上表明该方法显著改进了最先进的n-gram模型，并且该方法允许利用更长的上下文。

---
