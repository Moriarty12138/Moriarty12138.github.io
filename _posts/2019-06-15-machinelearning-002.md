---
title: 【西瓜书】 002 模型评估与选择
date: 2019-06-15 16:26:00
---

### 经验误差与过拟合

错误率（error rate）：分类错误的样本数占总样本总数的比例

误差（error）：学习器的实际预测输出与样本的真实输出之间的差异

训练误差（train error）/经验误差（empirical error）：学习器在训练集上的误差

泛化误差（generalization error）：学习器在新样本上的误差


只能使经验误差最小化从而得到泛化误差小的学习器。


当学习器把训练样本学得太好的话，和有可能把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，导致泛化性能下降。
这种现象在机器学习中被称为过拟合。
与过拟合相对的是欠拟合，这是指对训练样本的一般性质尚未学习好。



导致过拟合的因素有很多，最常见的情况是由于学习能力过于强大，以至于把训练样本所包含的不太一样的特性都学习到了，而欠拟合则通常是由于学习能力地下而导致的。

欠拟合比较容易克服，增加学习能力（拓展决策树的分支，增加神经网络训练轮数）。

各类算法都必然带有一些针对过拟合的措施；
然而必须意识到，过拟合是无法彻底避免的，我们做能做到的只有缓解。

机器学习面临的问题通常是NP甚至更难，
而有效的学习算法是在多项式时间内运行完成，
若可彻底避免过拟合，
则通过经验误差最小化就能获得最优解，
这就意味着我们构造性地证明了P=NP；
因此，只要相信P!=NP过拟合就不可避免。


### 评估方法

通常需要使用一个测试机来测试学习其对新样本的判别能力，
然后以测试误差作为泛化误差的近似。
通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。
测试集应该近可能与训练集互斥。


常见训练集与测试集处理方法
1. 留出法
将数据集划分为两个互斥的集合，作为训练集和测试集。
划分要尽可能保持数据分析的一致性。
（分类问题要保持样本的类别相似性）
即使在给定训练/测试集的样本比例后，让存在多种划分方式对初始数据集进行分割。
单次使用留出法得到的结果往往不够稳定，在使用留出法时，一般要采取若干次随即划分、重复进行实验评估后取平均值作为留出法的评估结果。
若令训练集包含绝大多数样本，则训练出的模型更接近于数据集，由于测试集比较小，评估结果不够稳定准确；
若测试机包含更多样本，则训练集和测试集差别更大了，评估的模型与用数据集训练出的模型相比可能有较大的差别，从而降低了评估模型的保真性。
这个问题没有完美的解决方案，常见做法是将2/3~4/5的样本用于训练，剩余样本用于测试。
2. 交叉验证法

3. 自助法
4. 调参与最终模型


### 性能度量（performance measure）

在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果；
这意味着模型的好坏是相对的，不仅取决于算法和数据还取决于任务需求。

回归任务最常用的性能度量是均方误差（mean squared error）：
$$E(f;D) = \frac{1}{m} \sum_{i=1}^{m}{(f(x_i) - y_i)^2}$$

更一般的，对于数据分布D和概率密度函数p(.)，均方误差可描述为：
$$E(f;D) = \f_{x~D}{(f(x) - y)^2 P(x) dx}$$

#### 错误率与精度

错误率和精度是分类任务常用的两种性能度量，
即适用于二分类任务，也适用于多分类任务。

错误率是分类错误的样本数占样本总数的比例。

精度是分类正确的样本数占样本总数的比例。

对样例D，分类错误率定义为
$$E(f;D) = \frac{1}{m}\sum_{i=1}{m}{}$$


#### 查准率、查全率与F1

#### ROC与AUC

#### 代价敏感错误率与代价曲线


### 比较检验

#### 假设检验

#### 交叉验证t检验

#### McNemar检验

#### Firedman检验与Nemenyi后续检验


### 偏差与方差
