---
title: 【论文研读】 002 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
date: 2019-07-03 19:13:00
---

author:Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova  
Google AI Language  
{jacobdevlin,mingweichang,kentonl,kristout}@google.com  


### 论文动机？

预训练的语言模型可以很好的提高许多自然语言处理的任务，无论是在句子级的任务还是词级别的任务。

目前有两种将预训练语言表示应用在下游任务的策略: feature-based和fine-turning
1. ELMo是feature-based
2. OpenAI GPT是fine-turning

这两种策略在预训练时有相同的目标函数，它们使用单向的语言模型来学习通用的语言表示。


作者认为目前的技术限制了预训练的表示，尤其是fine-turning的方法。
主要的限制来源于传统的语言模型是单向的，而且语言模型限制了在预训练阶段可以使用的模型结构的选择。


### Bert 相对于 word2vec 、 glove 、ELMo、GPT 的优点？



### Bert 采用的模型？
Bidirectional Encoder Representations from Transformers

BERT使用masked language model（MLM）的预训练目标（Cloze task完形填空）减弱之前单向表示的限制。

MLM随机遮住一些token，通过上下文来预测这个原始单词的id。

除了MLM，作者也设计的“next sentence prediction”的任务，这个任务关联预训练文本对表示。


### Bert 属于  Feature-based or fine-turning?



### Bert 如何进行特征拼接？



### Bert 解决了什么问题？

1. 证明了双向预训练对语言表示的重要性。
2. 与训练表示能够减少许多对于特定任务精心设计体系结构的需求。
BERT是第一个能够在句子级和token级实现了最先进的性能，并且由于许多特定任务的模型结构的基于fine-turning的表示模型
3. BERT在11项NLP任务上取得了最新的成绩。

### 用一句话介绍 Bert ？



### Bert 模型怎么应用到下游任务？

### Bert 存在问题？

### 如何评价 Bert ？
