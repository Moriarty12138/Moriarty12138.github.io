---
title: 【论文研读】 002 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
date: 2019-07-03 19:13:00
---

author:Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova  
Google AI Language  
{jacobdevlin,mingweichang,kentonl,kristout}@google.com  


### 论文动机？

预训练的语言模型可以很好的提高许多自然语言处理的任务，无论是在句子级的任务还是词级别的任务。

目前有两种将预训练语言表示应用在下游任务的策略: feature-based和fine-tuning
1. ELMo是feature-based
2. OpenAI GPT是fine-tuning

这两种策略在预训练时有相同的目标函数，它们使用单向的语言模型来学习通用的语言表示。


作者认为目前的技术限制了预训练的表示，尤其是fine-tuning的方法。
主要的限制来源于传统的语言模型是单向的，而且语言模型限制了在预训练阶段可以使用的模型结构的选择。


### BERT结构
Bidirectional Encoder Representations from Transformers

BERT使用masked language model（MLM）的预训练目标（Cloze task完形填空）减弱之前单向表示的限制。

MLM随机遮住一些token，通过上下文来预测这个原始单词的id。

除了MLM，作者也设计的“next sentence prediction”的任务，这个任务关联预训练文本对表示。


BERT框架分为两步:pre-training和fine-tuning
1. 在pre-training使用无标签数据进行不同的预训练任务。
2. 在fine-tuning使用预训练参数初始化BERT模型，然后使用下游任务中的标记数据对所有参数进行微调。
每个下游任务都有单独的微调模型。


BERT一个特别的特点是对于每个不同的任务有统一的结果。
预训练结构和下游结构有着很小的差异。


BERT的模型结构是多层双向Transformer编码器。

表示层（Transformer块）数量为L，隐藏层尺寸为H，自注意头的个数为A。
将前馈/过滤器的大小设置为4H

* $BERT_{BASE}$: L=12, H=768, TotalParameters=110M
* $BERT_{LARGE}$: L=24, H=1024, TotalParameters=340M


输入表示能够在一个标记序列中清楚地表示单个文本句子或一对文本句子，
通过把给定标记对应的标记嵌入、句子嵌入和位置嵌入求和来构造其输入表示。

![ml-02-3](/images/DL-images/bert-input.jpg)


#### BERT预训练任务

1. Masked LM
2. Next Sentence Prediction(NSP)


#### fine-tuning BERT

对于每个任务，我们简单的将特定任务的输入和输出插入BERT，端到端的fine-tuning所有参数。


### Bert 属于  Feature-based or fine-tuning?

fine-tuning


### Bert 解决了什么问题？

1. 证明了双向预训练对语言表示的重要性。
2. 与训练表示能够减少许多对于特定任务精心设计体系结构的需求。
BERT是第一个能够在句子级和token级实现了最先进的性能，并且由于许多特定任务的模型结构的基于fine-tuning的表示模型
3. BERT在11项NLP任务上取得了最新的成绩。


### 如何评价 Bert ？

最近，由于使用语言模型进行迁移学习而取得的实验提升表明，丰富的、无监督的预训练是许多语言 理解系统不可或缺的组成部分。特别是，这些结果使得即使是低资源(少量标签的数据集)的任务也 能从非常深的单向结构模型中受益。我们的主要贡献是将这些发现进一步推广到深层的双向结构，使 同样的预训练模型能够成功地广泛地处理 NLP 任务。

虽然这些实证结果很有说服力，在某些情况下甚至超过了人类的表现，但未来重要的工作是研究 BERT 可能捕捉到的或不捕捉到的语言现象。
