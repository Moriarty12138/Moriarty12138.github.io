---
title: 【NLP】面经总结
date: 2018-12-25 14:38:00
---

冲鸭！
---
### NLP

1. 都用过那些模型实现文本分类，总结一下各个模型的优缺点

统计模型：Naive Bayes、XGboost
神经网络：CNN、LSTM、GRU

朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用，用mRMR中R来讲，就是特征冗余。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。

优点：
  1. 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率；
  2. 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已；
  3. 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）；
  4. 对缺失数据不太敏感，算法也比较简单，常用于文本分类；
  5. 朴素贝叶斯对结果解释容易理解。

缺点：
  1. 需要计算先验概率；
  2. 分类决策存在错误率；
  3. 对输入数据的表达形式很敏感；
  4. 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。

Adaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。该算法是一种典型的boosting算法，其加和理论的优势可以使用Hoeffding不等式得以解释。

优点
  1. Adaboost是一种有很高精度的分类器；
  2. 可以使用各种方法构建子分类器，Adaboost算法提供的是框架；
  3. 当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单；
  4. 简单，不用做特征筛选；
  5. 不易发生overfitting。

缺点
  1. 对outlier比较敏感。

支持向量机，一个经久不衰的算法，高准确率，为避免过拟合提供了很好的理论保证，而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行得很好。在动辄超高维的文本分类问题中特别受欢迎。可惜内存消耗大，难以解释，运行和调参也有些烦人，而随机森林却刚好避开了这些缺点，比较实用。

优点
  1. 可以解决高维问题，即大型特征空间；
  2. 解决小样本下机器学习问题；
  3. 能够处理非线性特征的相互作用；
  4. 无局部极小值问题；（相对于神经网络等算法）
  5. 无需依赖整个数据；
  6. 泛化能力比较强。

缺点

  1. 当观测样本很多时，效率并不是很高；
  2. 对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数；
  3. 对于核函数的高维映射解释力不强，尤其是径向基函数；
  4. 常规SVM只支持二分类；
  5. 对缺失数据敏感。

对于核的选择也是有技巧的（libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核）：
  1. 如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了；也可以先对数据进行降维，然后使用非线性核，这也是一种方法。
  2. 如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果；
  3. 如果样本数目和特征数目相等，该情况可以使用非线性核，原理和第二种一样。

人工神经网络的优点：
  1. 分类的准确度高；
  2. 并行分布处理能力强,分布存储及学习能力强；
  3. 对噪声神经有较强的鲁棒性和容错能力；
  4. 具备联想记忆的功能，能充分逼近复杂的非线性关系。

人工神经网络的缺点：
  1. 神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值；
  2. 黑盒过程，不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度；
  3. 学习时间过长，有可能陷入局部极小值，甚至可能达不到学习的目的。

RNN 最大的优势是其天生的具有时序结构，十分适合解决NLP问题。NLP的输入往往是个不定长的线性序列句子，而RNN本身结构就是个可以接纳不定长输入的由前向后进行信息线性传导的网络结构，而在LSTM引入三个门后，对于捕获长距离特征也是非常有效的。
RNN 最大的缺点在于反向传播时所存在的优化困难问题， 即梯度消失，梯度爆炸问题，当然，后来的LSTM 与 GRU 相当程度上解决了这一问题。后来陆续有研究对其优化来解决 RNN 的长期依赖问题。RNN 的另一大缺陷在于其并行能力。由于每一时刻状态的生成都依赖于上一时刻的状态，这使得 RNN 无法并行。

CNN 能够捕捉到 n-gram 信息， filter 的size 决定了n的大小。
只有一个卷积层带来的问题是：对于远距离特征，单层CNN是无法捕获到的，如果滑动窗口k最大为2，而如果有个远距离特征距离是5，那么无论上多少个卷积核，都无法覆盖到长度为5的距离的输入，所以它是无法捕获长距离特征的。
那么怎样才能捕获到长距离的特征呢？有两种典型的改进方法：
一种是假设我们仍然用单个卷积层，滑动窗口大小k假设为3，就是只接收三个输入单词，但是我们想捕获距离为5的特征，那么采用 Dilated 卷积思想， 跳着覆盖。
第二种方法是把深度做起来。第一层卷积层，假设滑动窗口大小k是3，如果再往上叠一层卷积层，假设滑动窗口大小也是3，但是第二层窗口覆盖的是第一层窗口的输出特征，所以它其实能覆盖输入的距离达到了5。如果继续往上叠加卷积层，可以继续增大卷积核覆盖输入的长度。
回头看Kim版本CNN还有一个问题，就是那个Max Pooling层，这块其实与CNN能否保持输入句子中单词的位置信息有关系。CNN是否能够保留原始输入的相对位置信息呢？
其实CNN的卷积核是能保留特征之间的相对位置的，道理很简单，滑动窗口从左到右滑动，捕获到的特征也是如此顺序排列，所以它在结构上已经记录了相对位置信息了。但是如果卷积层后面立即接上Pooling层的话，Max Pooling的操作逻辑是：从一个卷积核获得的特征向量里只选中并保留最强的那一个特征，所以到了Pooling层，位置信息就被扔掉了，这在NLP里其实是有信息损失的。所以在NLP领域里，目前CNN的一个发展趋势是抛弃Pooling层，靠全卷积层来叠加网络深度，这背后是有原因的。
想方设法把CNN的深度做起来，随着深度的增加，很多看似无关的问题就随之解决了。

Transformer 的优劣
不定长问题： 一般设定输入的最大长度，如果句子没那么长，则用Padding填充，这样整个模型输入起码看起来是定长的了。
位置信息： Transformer是用位置函数来进行位置编码的，而Bert等模型则给每个单词一个Position embedding，将单词embedding和单词对应的position embedding加起来形成单词的输入embedding。
长距离依赖问题： Self attention天然就能解决这个问题，因为在集成信息的时候，当前单词和句子中任意单词都发生了联系。
Transformer 的缺陷：
对于长输入的任务，典型的比如篇章级别的任务（例如文本摘要），因为任务的输入太长，Transformer会有巨大的计算复杂度，导致速度会急剧变慢。
Transformer整体结构确实显得复杂了一些，如何更深刻认识它的作用机理，然后进一步简化它，这也是一个好的探索方向。

三大特征抽取器比较
语义特征抽取能力： Transoformer 显著超越 RNN 和 CNN， RNN 和 CNN 差不多。
长距离特征捕获能力： CNN 显著弱于 RNN 和 Transformer。 Transformer微弱优于 RNN（主语谓语距离小于13），但在比较远的距离上（主语谓语距离大于13），RNN微弱优于Transformer。对于Transformer来说，Multi-head attention的head数量严重影响NLP任务中Long-range特征捕获能力：结论是head越多越有利于捕获long-range特征。
任务综合特征抽取能力： Transformer 强于 RNN 和 CNN
并行计算能力： Transformer 与 CNN 差不多，都远强于 RNN。
计算量： Transformer Block > CNN > RNN
训练速度： Transformer Base > CNN > Transformer Big > RNN
单从任务综合效果方面来说，Transformer明显优于CNN，CNN略微优于RNN。速度方面Transformer和CNN明显占优，RNN在这方面劣势非常明显。这两者再综合起来，如果我给的排序结果是Transformer>CNN>RNN.


2. 文本分类样本不平衡问题

  1. 对数据相对不足的类别上采样（oversampling the minority)
  2. 对数据相对过多的下采样（under-sampling the majority)
  3. SMOTE (Synthetic Minority Oversampling Technique) 对数据较少的类别人工采样
  4. 阈值调整（threshold moving），将原本默认为0.5的阈值调整到 较少类别/（较少类别+较多类别）即可 训练集是总体样本的无偏采样，观测几率就代表真实几率
  5. 对数据先进行聚类，再将大的簇进行随机欠采样或者小的簇进行数据生成
  6. 一分类

3. 文本分类多标签问题

已有的多标记学习算法的策略思路大致可以分为以下三类[[8]]：
  1. “一阶（first-order）”策略：该类策略通过逐一考察单个标记而忽略标记之间的相关性，如将多标记学习问题分解为个独立的二类分类问题，从而构造多标记学习系统。该类方法效率较高且实现简单，但由于其完全忽略标记之间可能存在的相关性，其系统的泛化性能往往较低。
  2. “二阶（second-order）”策略：该类策略通过考察两两标记之间的相关性，如相关标记与无关标记之间的排序关系，两两标记之间的交互关系等等，从而构造多标记学习系统。该类方法由于在一定程度上考察了标记之间的相关性，因此其系统泛化性能较优。
  3. “高阶（high-order）”策略：该类策略通过考察高阶的标记相关性，如处理任一标记对其它所有标记的影响，处理一组随机标记集合的相关性等等，从而构造多标记学习系统。该类方法虽然可以较好地反映真实世界问题的标记相关性，但其模型复杂度往往过高，难以处理大规模学习问题。

基本来讲，解决多标签分类问题有3种方法，分别是：
  1. 问题转换 二元关联、分类器链、LP法
  2. 自适应算法
  3. 集成方法


4. 基于知识库的对话系统的结构和工作原理？

5. 句子相似度的做法

句子相似度计算方法：
  1. 编辑距离计算
  2. 杰卡德系数计算
  3. TF 计算
  4. TFIDF 计算
  5. Word2Vec 计算

6. 文本分类有什么传统方法

常用分类算法的思路包括下面四种：
  1. 朴素贝叶斯分类器：利用特征项和类别的联合概率来估计文本的类别概率；
  2. 支持向量机分类器：在向量空间中找到一个决策平面，这个平面能够最好的切割两个分类的数据点，主要用于解决二分类问题；
  3. KNN 方法：在训练集中找到离它最近的 k 个临近文本，并根据这些文本的分类来给测试文档分类；
  4. 决策树方法：将文本处理过程看作是一个等级分层且分解完成的复杂任务。

7. word2vec的原理与改进方式；

https://www.cnblogs.com/pinard/p/7160330.html

  1. Hierarchical Softmax
  2. Negative Sampling


8. NLP基础任务，比如分词算法（序列标注任务），分类算法


### 机器学习

1. GBDT的原理

GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，又叫 MART（Multiple Additive Regression Tree)，它通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出。该算法将决策树与集成思想进行了有效的结合。

由于GBDT的核心在与累加所有树的结果作为最终结果，而分类树得到的离散分类结果对于预测分类并不是这么的容易叠加（稍等后面会看到，其实并不是简单的叠加，而是每一步每一棵树拟合的残差和选择分裂点评价方式都是经过公式推导得到的），而对基于回归树所得到的数值进行加减是有意义的（例如10岁+5岁-3岁=12岁），这是区别于分类树的一个显著特征（毕竟男+女=是男是女?，这样的运算是毫无道理的），GBDT在运行时就使用到了回归树的这个性质，它将累加所有树的结果作为最终结果。所以GBDT中的树都是回归树，而不是分类树，它用来做回归预测，当然回归树经过调整之后也能用来做分类。

梯度提升决策树GBDT是Boosting算法。
基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。
算法的基本流程：在每一轮迭代中，首先计算出当前模型在所有样本上的负梯度，然后以该值为目标训练一个新的弱分类器进行拟合并计算出该弱分类器的权重，最终实现对模型的更新。

[机器学习算法系列（7）：GBDT](https://plushunter.github.io/2017/01/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%887%EF%BC%89%EF%BC%9AGBDT/)


2. 决策树节点分裂时如何选择特征，写出Gini index和information Gain的公式并举例说明

若利用一个特征进行分类的结果与随机分类的结果没有很大差异，则称这个特征是没有分类能力的。特征选择的准则是信息增益或信息增益比。直观上，若一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割为子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益可以表示这一直观的准则。

[机器学习算法系列（4）：决策树](https://plushunter.github.io/2017/01/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91/)

3. 分类树和回归树的区别是什么

分类树使用信息增益或增益比率来划分节点；每个节点样本的类别情况投票决定测试样本的类别。
回归树使用最大均方差划分节点；每个节点样本的均值作为测试样本的回归预测值。

4. 与Random Forest做比较，并以此介绍什么是模型的Bias和Variance

当我们谈论机器学习模型的误差的时候，这个误差可以主要分为两部分，bias和variance。一般情况下，模型需要在bias和variance之间取得一个平衡。bias小的模型，variance一般大；variance小的模型，bias一般大。更好的理解bias和variance的关系能够帮助我们更好的应付模型的过拟合和欠拟合问题。

Error due to Bias: Bias表示的就是模型预测的值和真实值之间的距离的期望。所以我们会通过建立多个模型（如使用不同的数据子集）来估计这个误差期望值。Bias代表着算法的拟合能力。

Error due to Variance: Variance表示的是当你对一个模型使用不同的数据进行多次建模时，这些模型在某一个点上的预测值的方差就是该模型在这个点上预测值的variance。其实就是预测值的方差的意思。Variance代表这算法的鲁棒性。

我们发现，通过对这n个模型进行平均，variance减小了，但bias没变。这就是bagging的功效，也就是为什么random forest更加鲁棒的原因（也可以用来解释神经网络中的dropout）。虽然bagging可以减小模型的方差，但是我们发现，并没有增加模型预测的准确率，也就是bagging减小了variance，却没减小bias。因而bagging中的模型是强模型，bias小，variance大。使用bagging，目标是降低variance。

为了减小模型的bias，我们可以使用boosting的方法（如adaBoost，GBDT，XGBoost)。Boosting中的模型是弱模型（比随机猜测好一点），bias大，variance小。如adaBoost在实现的时候常用决策树桩（也就是单层的决策树）。boosting的方法，会在之前模型预测误差即基础上继续建模预测，因而能够减小模型的bias，但由于此时模型之间不是独立的，所以variance的减小不显著。

[机器学习中的Bias和Variance](https://zhuanlan.zhihu.com/p/45213397)


5. XGBoost的参数调优有哪些经验

常用参数解读：

estimator：所使用的分类器，如果比赛中使用的是XGBoost的话，就是生成的model。比如： model = xgb.XGBRegressor(\**other_params)
param_grid：值为字典或者列表，即需要最优化的参数的取值。比如：cv_params = {'n_estimators': [550, 575, 600, 650, 675]}
scoring :准确度评价标准，默认None,这时需要使用score函数；或者如scoring='roc_auc'，根据所选模型不同，评价准则不同。字符串（函数名），或是可调用对象，需要其函数签名形如：scorer(estimator, X, y)；如果是None，则使用estimator的误差估计函数。

最佳迭代次数：n_estimators 从粗粒度到细粒度

每次调完一个参数，要把 other_params对应的参数更新为最优值。

选择较高的学习速率(learning rate)。一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。选择对应于此学习速率的理想决策树数量。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。

对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。在确定一棵树的过程中，我们可以选择不同的参数，待会儿我会举例说明。

xgboost的正则化参数的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。

降低学习速率，确定理想参数。

6. XGBoost的正则化是如何实现的

$l=rT+\frac{1}{2}\ri\sum_{j=1}^{T}{f_t}$

7. XGBoost的并行化部分是如何实现的

GBDT每棵树基于前一个树得到，不能并行。
XGBoost Gi和Hi同时进行，树生成时特征分类也进行了提前计算实现并行计算。

8. 如果选用一种其他的模型替代XGBoost，会用什么

LightGBM
LightGBM是一个梯度Boosting框架，使用基于决策树的学习算法。它可以说是分布式的，高效的，有以下优势：
1）更快的训练效率
2）低内存使用
3）更高的准确率
4）支持并行化学习
5）可以处理大规模数据

关于XGboost的不足之处主要有：
1）每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。
2）预排序方法的时间和空间的消耗都很大

https://www.cnblogs.com/jiangxinyang/p/9337094.html

9. L1与L2的作用；

L1正则化和L2正则化可以看做是损失函数的惩罚项。
所谓『惩罚』是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。

L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合


10. SVM的原理；

手撕SVM

11. bagging与boosting方法的关系与区别

bagging的算法过程如下：
从原始样本集中使用Bootstraping 方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集（k个训练集之间相互独立，元素可以有重复）。
对于n个训练集，我们训练k个模型，（这个模型可根据具体的情况而定，可以是决策树，knn等）
对于分类问题：由投票表决产生的分类结果；对于回归问题，由k个模型预测结果的均值作为最后预测的结果（所有模型的重要性相同）。

Boosting（提升法）
boosting的算法过程如下：
对于训练集中的每个样本建立权值wi，表示对每个样本的权重， 其关键在与对于被错误分类的样本权重会在下一轮的分类中获得更大的权重（错误分类的样本的权重增加）。
同时加大分类 误差概率小的弱分类器的权值，使其在表决中起到更大的作用，减小分类误差率较大弱分类器的权值，使其在表决中起到较小的作用。每一次迭代都得到一个弱分类器，需要使用某种策略将其组合，最为最终模型，(adaboost给每个迭代之后的弱分类器一个权值，将其线性组合作为最终的分类器,误差小的分类器权值越大。)

Bagging和Boosting 的主要区别
样本选择上: Bagging采取Bootstraping的是随机有放回的取样，Boosting的每一轮训练的样本是固定的，改变的是买个样的权重。
样本权重上：Bagging采取的是均匀取样，且每个样本的权重相同，Boosting根据错误率调整样本权重，错误率越大的样本权重会变大
预测函数上：Bagging所以的预测函数权值相同，Boosting中误差越小的预测函数其权值越大。
并行计算: Bagging 的各个预测函数可以并行生成;Boosting的各个预测函数必须按照顺序迭代生成.


12. 控制过拟合的方式；

threshold out 把训练集分为训练集和验证集，其中训练集用来训练数据，验证集用来检测准确率。
我们在每个迭代期的最后都计算在验证集上的分类准确率，一旦分类准确率已经饱和，就停止训练。这个策略被称为提前停止。

增大训练量

规范化（regularization）

Dropout
Dropout是一种相当激进的技术，和之前的规范化技术不同，它不改变网络本身，而是会随机地删除网络中的一般隐藏的神经元，并且让输入层和输出层的神经元保持不变。

我们每次使用梯度下降时，只使用随机的一般神经元进行更新权值和偏置，因此我们的神经网络时再一半隐藏神经元被丢弃的情况下学习的。

而当我们运行整个网络时，是两倍的神经元会被激活。因此，我们将从隐藏神经元的权重减半。

这种技术的直观理解为：当我们Dropout不同的神经元集合时，有点像我们在训练不同的神经网络。而不同的神经网络会以不同的方式过拟合，所以Dropout就类似于不同的神经网络以投票的方式降低过拟合。

对于不同的技术，其实都可以理解为：我们在训练网络的健壮性。无论是L1、L2规范化倾向于学习小的权重，还是Dropout强制学习在神经元子集中更加健壮的特征，都是让网络对丢失个体连接的场景更加健壮。

13. Xgboost相较boosting方式的优势，以及lightGBM相较Xgboost的提升

第8条

14. 朴素贝叶斯

手撕naive baye

15. 逻辑回归，线性回归

手撕LR

16. 决策树，不同的划分方式，ID3，C4.5，CTAR，XGBoost等等

手撕DT

17. Ensemble模型

11

18. SVM，核函数选择，不同SVM形式

10

19. HMM，CRF，如何轻松愉快地理解条件随机场（CRF）？

手撕HMM CRF

20. 最大熵原理，图解最大熵原理（The Maximum Entropy Principle）

手撕最大熵

21. KNN和K-Means，DBSACN也了解一下，以及各种距离计算方式，关于机器学习距离的理解

手撕knn kmean

#### 深度学习

1. 手写损失函数

损失函数（Loss Function）：是定义在单个样本上的，是指一个样本的误差。
代价函数（Cost Function）：是定义在整个训练集上的，是所有样本误差的平均，也就是所有损失函数值的平均。
目标函数（Object Function）：是指最终需要优化的函数，一般来说是经验风险+结构风险，也就是（代价函数+正则化项）。

1）0-1损失函数
2）平方损失函数
3）绝对值损失函数
4）对数损失函数

2. 介绍了一下attention机制，手写attention的公式，怎样计算得分和权重，soft attention和hard attention的区别

注意力机制即 Attention mechanism在序列学习任务上具有巨大的提升作用，在编解码器框架内，通过在编码段加入Attention模型，对源数据序列进行数据加权变换，或者在解码端引入Attention 模型，对目标数据进行加权变化，可以有效提高序列对序列的自然方式下的系统表现。

为什么要加入Attention：
当输入序列非常长时，模型难以学到合理的向量表示
序列输入时，随着序列的不断增长，原始根据时间步的方式的表现越来越差，这是由于原始的这种时间步模型设计的结构有缺陷，即所有的上下文输入信息都被限制到固定长度，整个模型的能力都同样收到限制，我们暂且把这种原始的模型称为简单的编解码器模型。
编解码器的结构无法解释，也就导致了其无法设计。

长输入序列带来的问题：
使用传统编码器-解码器的RNN模型先用一些LSTM单元来对输入序列进行学习，编码为固定长度的向量表示；然后再用一些LSTM单元来读取这种向量表示并解码为输出序列。
采用这种结构的模型在许多比较难的序列预测问题（如文本翻译）上都取得了最好的结果，因此迅速成为了目前的主流方法。
这种结构在很多其他的领域上也取得了不错的结果。然而，它存在一个问题在于：输入序列不论长短都会被编码成一个固定长度的向量表示，而解码则受限于该固定长度的向量表示。

这个问题限制了模型的性能，尤其是当输入序列比较长时，模型的性能会变得很差（在文本翻译任务上表现为待翻译的原始文本长度过长时翻译质量较差）。

Attention机制的基本思想是：打破了传统编码器-解码器结构在编解码时都依赖于内部一个固定长度向量的限制。
Attention机制的实现是 通过保留LSTM编码器对输入序列的中间输出结果，然后训练一个模型来对这些输入进行选择性的学习并且在模型输出时将输出序列与之进行关联。

换一个角度而言，输出序列中的每一项的生成概率取决于在输入序列中选择了哪些项。

Attention-based Model 其实就是一个相似性的度量，当前的输入与目标状态约相似，那么在当前的输入的权重就会越大。就是在原有的model上加入了Attention的思想。


Attention-based Model其实就是一个相似性的度量，当前的输入与目标状态越相似，那么在当前的输入的权重就会越大，说明当前的输出越依赖于当前的输入。严格来说，Attention并算不上是一种新的model，而仅仅是在以往的模型中加入attention的思想，所以Attention-based Model或者Attention Mechanism是比较合理的叫法，而非Attention Model。



https://zhuanlan.zhihu.com/p/35739040

3. Seq2Seq的结构

4. fasttext、CNN、RNN的优缺点各是什么

5. 卷积神经网络相较于传统神经网络的优势；

6. 常用激活函数以及其变种；

7. LSTM、GRU是如何解决梯度消失问题的；

8. 深度神经网络是如何解决过拟合问题的；

9. 介绍下attention model的基本原理与应用场景；

10. 讲讲你对bert模型的理解；

11. CNN原理，如何用在文本上，在什么情况下适合用CNN，在什么情况下用LSTM

12. RNN系列，掌握RNN、LSTM和GRU的内部结构，RNN产生梯度消失的原因，LSTM如何解决，GRU对LSTM的改进

13. Word2vec工具，怎么训练词向量，skip-gram和cbow
https://zhuanlan.zhihu.com/p/35500923

14. 数据预处理，权重初始化，为什么不能全部初始化为0，词向量怎么预训练

15. 过拟合问题，原因是什么，怎么解决，主要从数据和模型两方面出发：机器学习中用来防止过拟合的方法有哪些？

16. 调参技巧，比如，卷积核大小怎么按层设置，bn放在哪里比较合适，激活函数之间的区别（sigmoid，tanh和relu），词向量维度怎么设置，等等。

17. 模型评估指标，acc，pre，recall，f1，roc曲线和auc曲线，分别适用于什么任务，怎么降低偏差，怎么降低方差，可以关注一下Hulu微信公众号：Hulu机器学习问题与解答系列 | 第一弹：模型评估

18. 优化方法，批量梯度下降，随机梯度下降，mini-batch梯度下降的区别，adam，adagrad，adadelta，牛顿法

19. 梯度消失问题，原因（链式求导，激活函数），解决方法（主要是batch norm）；以及梯度爆炸问题（梯度截断）

20. 关于训练集和验证集，为什么要划分，如何划分(留出法，交叉验证)

21. 如何处理数据不均衡问题，也是从数据和模型两方面出发解决。

22. BP后向传播过程的推导，可以参考：漫谈LSTM系列的梯度问题，先定义Loss函数，然后分别对输出层参数和隐藏层参数进行求导，得到参数的更新量。

21. softmax和交叉熵推导，分成i=j 和 ij 两种情况来算，参考这里：大师网-简单易懂的softmax交叉熵损失函数求导

22. 各种Loss函数

1

23. 似然函数，负对数似然函数的推导

23. 最小二乘法，利用矩阵的秩进行推导

24. 贝叶斯定理，拉普拉斯平滑

25. RNN在BP过程中梯度消失的原因，也把这个链式求导过程写出来。

26. 各种优化方法的公式，SGD，Momentum，Adagrad，Adam，机器学习优化方法总结比较 - 合唱团abc - 博客园

27. Batch Normalization，就是个归一化过程，再加一个scale操作

28. SVM推导，拉格朗日了解一下：机器学习之拉格朗日乘数法

29. 最大熵模型相关推导，一步一步理解最大熵模型 - wxquare - 博客园

30. pyTorch多线程

31. BERT

32. XLNet Transformer-XL

---
### 算法题

1. 算法题：股票买卖获最大利润

2. 复杂链表的复制，链表的删除

3. 最长公共子序列，逆序对

4. 快排，归并排序，堆排序

5. 二分查找，以及衍生的题目

6. 深度优先搜索
