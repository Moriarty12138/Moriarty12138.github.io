---
title: 【NLP】面经总结
date: 2018-12-25 14:38:00
---

冲鸭！
---
### NLP

1. 都用过那些模型实现文本分类，总结一下各个模型的优缺点

2. 文本分类样本不平衡问题

3. 文本分类多标签问题

4. 基于知识库的对话系统的结构和工作原理？



5. 句子相似度的做法

6. 文本分类有什么传统方法

7. word2vec的原理与改进方式；

8. NLP基础任务，比如分词算法（序列标注任务），分类算法

### 机器学习

1. GBDT的原理

2. 决策树节点分裂时如何选择特征，写出Gini index和information Gain的公式并举例说明

3. 分类树和回归树的区别是什么

4. 与Random Forest做比较，并以此介绍什么是模型的Bias和Variance

5. XGBoost的参数调优有哪些经验

6. XGBoost的正则化是如何实现的

7. XGBoost的并行化部分是如何实现的

8. 如果选用一种其他的模型替代XGBoost，会用什么

9. L1与L2的作用；

10. SVM的原理；

11. bagging与boosting方法的关系与区别

12. 控制过拟合的方式；

13. Xgboost相较boosting方式的优势，以及lightGBM相较Xgboost的提升

14. 朴素贝叶斯

15. 逻辑回归，线性回归

16. 决策树，不同的划分方式，ID3，C4.5，CTAR，XGBoost等等

17. Ensemble模型

18. SVM，核函数选择，不同SVM形式

19. HMM，CRF，如何轻松愉快地理解条件随机场（CRF）？

20. 最大熵原理，图解最大熵原理（The Maximum Entropy Principle）

21. KNN和K-Means，DBSACN也了解一下，以及各种距离计算方式，关于机器学习距离的理解


#### 深度学习

1. 手写损失函数

2. 介绍了一下attention机制，手写attention的公式，怎样计算得分和权重，soft attention和hard attention的区别
https://zhuanlan.zhihu.com/p/35739040
3. Seq2Seq的结构

4. fasttext、CNN、RNN的优缺点各是什么

5. 卷积神经网络相较于传统神经网络的优势；

6. 常用激活函数以及其变种；

7. LSTM、GRU是如何解决梯度消失问题的；

8. 深度神经网络是如何解决过拟合问题的；

9. 介绍下attention model的基本原理与应用场景；

10. 讲讲你对bert模型的理解；

11. CNN原理，如何用在文本上，在什么情况下适合用CNN，在什么情况下用LSTM

12. RNN系列，掌握RNN、LSTM和GRU的内部结构，RNN产生梯度消失的原因，LSTM如何解决，GRU对LSTM的改进

13. Word2vec工具，怎么训练词向量，skip-gram和cbow
https://zhuanlan.zhihu.com/p/35500923

14. 数据预处理，权重初始化，为什么不能全部初始化为0，词向量怎么预训练

15. 过拟合问题，原因是什么，怎么解决，主要从数据和模型两方面出发：机器学习中用来防止过拟合的方法有哪些？

16. 调参技巧，比如，卷积核大小怎么按层设置，bn放在哪里比较合适，激活函数之间的区别（sigmoid，tanh和relu），词向量维度怎么设置，等等。

17. 模型评估指标，acc，pre，recall，f1，roc曲线和auc曲线，分别适用于什么任务，怎么降低偏差，怎么降低方差，可以关注一下Hulu微信公众号：Hulu机器学习问题与解答系列 | 第一弹：模型评估

18. 优化方法，批量梯度下降，随机梯度下降，mini-batch梯度下降的区别，adam，adagrad，adadelta，牛顿法

19. 梯度消失问题，原因（链式求导，激活函数），解决方法（主要是batch norm）；以及梯度爆炸问题（梯度截断）

20. 关于训练集和验证集，为什么要划分，如何划分(留出法，交叉验证)

21. 如何处理数据不均衡问题，也是从数据和模型两方面出发解决。

22. BP后向传播过程的推导，可以参考：漫谈LSTM系列的梯度问题，先定义Loss函数，然后分别对输出层参数和隐藏层参数进行求导，得到参数的更新量。

21. softmax和交叉熵推导，分成i=j 和 ij 两种情况来算，参考这里：大师网-简单易懂的softmax交叉熵损失函数求导

22. 各种Loss函数

23. 似然函数，负对数似然函数的推导

23. 最小二乘法，利用矩阵的秩进行推导

24. 贝叶斯定理，拉普拉斯平滑

25. RNN在BP过程中梯度消失的原因，也把这个链式求导过程写出来。

26. 各种优化方法的公式，SGD，Momentum，Adagrad，Adam，机器学习优化方法总结比较 - 合唱团abc - 博客园

27. Batch Normalization，就是个归一化过程，再加一个scale操作

28. SVM推导，拉格朗日了解一下：机器学习之拉格朗日乘数法

29. 最大熵模型相关推导，一步一步理解最大熵模型 - wxquare - 博客园

30. pyTorch多线程

---
### 算法题

1. 算法题：股票买卖获最大利润

2. 复杂链表的复制，链表的删除

3. 最长公共子序列，逆序对

4. 快排，归并排序，堆排序

5. 二分查找，以及衍生的题目

6. 深度优先搜索
