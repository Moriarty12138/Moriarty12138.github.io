---
title: 【NLP】面经总结
date: 2018-12-25 14:38:00
---

冲鸭！
---
### NLP

1. 都用过那些模型实现文本分类，总结一下各个模型的优缺点

统计模型：Naive Bayes、XGboost
神经网络：CNN、LSTM、GRU

朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用，用mRMR中R来讲，就是特征冗余。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。

优点：
  1. 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率；
  2. 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已；
  3. 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）；
  4. 对缺失数据不太敏感，算法也比较简单，常用于文本分类；
  5. 朴素贝叶斯对结果解释容易理解。

缺点：
  1. 需要计算先验概率；
  2. 分类决策存在错误率；
  3. 对输入数据的表达形式很敏感；
  4. 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。

Adaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。该算法是一种典型的boosting算法，其加和理论的优势可以使用Hoeffding不等式得以解释。

优点
  1. Adaboost是一种有很高精度的分类器；
  2. 可以使用各种方法构建子分类器，Adaboost算法提供的是框架；
  3. 当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单；
  4. 简单，不用做特征筛选；
  5. 不易发生overfitting。

缺点
  1. 对outlier比较敏感。

支持向量机，一个经久不衰的算法，高准确率，为避免过拟合提供了很好的理论保证，而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行得很好。在动辄超高维的文本分类问题中特别受欢迎。可惜内存消耗大，难以解释，运行和调参也有些烦人，而随机森林却刚好避开了这些缺点，比较实用。

优点
  1. 可以解决高维问题，即大型特征空间；
  2. 解决小样本下机器学习问题；
  3. 能够处理非线性特征的相互作用；
  4. 无局部极小值问题；（相对于神经网络等算法）
  5. 无需依赖整个数据；
  6. 泛化能力比较强。

缺点

  1. 当观测样本很多时，效率并不是很高；
  2. 对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数；
  3. 对于核函数的高维映射解释力不强，尤其是径向基函数；
  4. 常规SVM只支持二分类；
  5. 对缺失数据敏感。

对于核的选择也是有技巧的（libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核）：
  1. 如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了；也可以先对数据进行降维，然后使用非线性核，这也是一种方法。
  2. 如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果；
  3. 如果样本数目和特征数目相等，该情况可以使用非线性核，原理和第二种一样。

人工神经网络的优点：
  1. 分类的准确度高；
  2. 并行分布处理能力强,分布存储及学习能力强；
  3. 对噪声神经有较强的鲁棒性和容错能力；
  4. 具备联想记忆的功能，能充分逼近复杂的非线性关系。

人工神经网络的缺点：
  1. 神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值；
  2. 黑盒过程，不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度；
  3. 学习时间过长，有可能陷入局部极小值，甚至可能达不到学习的目的。

RNN 最大的优势是其天生的具有时序结构，十分适合解决NLP问题。NLP的输入往往是个不定长的线性序列句子，而RNN本身结构就是个可以接纳不定长输入的由前向后进行信息线性传导的网络结构，而在LSTM引入三个门后，对于捕获长距离特征也是非常有效的。
RNN 最大的缺点在于反向传播时所存在的优化困难问题， 即梯度消失，梯度爆炸问题，当然，后来的LSTM 与 GRU 相当程度上解决了这一问题。后来陆续有研究对其优化来解决 RNN 的长期依赖问题。RNN 的另一大缺陷在于其并行能力。由于每一时刻状态的生成都依赖于上一时刻的状态，这使得 RNN 无法并行。

CNN 能够捕捉到 n-gram 信息， filter 的size 决定了n的大小。
只有一个卷积层带来的问题是：对于远距离特征，单层CNN是无法捕获到的，如果滑动窗口k最大为2，而如果有个远距离特征距离是5，那么无论上多少个卷积核，都无法覆盖到长度为5的距离的输入，所以它是无法捕获长距离特征的。
那么怎样才能捕获到长距离的特征呢？有两种典型的改进方法：
一种是假设我们仍然用单个卷积层，滑动窗口大小k假设为3，就是只接收三个输入单词，但是我们想捕获距离为5的特征，那么采用 Dilated 卷积思想， 跳着覆盖。
第二种方法是把深度做起来。第一层卷积层，假设滑动窗口大小k是3，如果再往上叠一层卷积层，假设滑动窗口大小也是3，但是第二层窗口覆盖的是第一层窗口的输出特征，所以它其实能覆盖输入的距离达到了5。如果继续往上叠加卷积层，可以继续增大卷积核覆盖输入的长度。
回头看Kim版本CNN还有一个问题，就是那个Max Pooling层，这块其实与CNN能否保持输入句子中单词的位置信息有关系。CNN是否能够保留原始输入的相对位置信息呢？
其实CNN的卷积核是能保留特征之间的相对位置的，道理很简单，滑动窗口从左到右滑动，捕获到的特征也是如此顺序排列，所以它在结构上已经记录了相对位置信息了。但是如果卷积层后面立即接上Pooling层的话，Max Pooling的操作逻辑是：从一个卷积核获得的特征向量里只选中并保留最强的那一个特征，所以到了Pooling层，位置信息就被扔掉了，这在NLP里其实是有信息损失的。所以在NLP领域里，目前CNN的一个发展趋势是抛弃Pooling层，靠全卷积层来叠加网络深度，这背后是有原因的。
想方设法把CNN的深度做起来，随着深度的增加，很多看似无关的问题就随之解决了。

Transformer 的优劣
不定长问题： 一般设定输入的最大长度，如果句子没那么长，则用Padding填充，这样整个模型输入起码看起来是定长的了。
位置信息： Transformer是用位置函数来进行位置编码的，而Bert等模型则给每个单词一个Position embedding，将单词embedding和单词对应的position embedding加起来形成单词的输入embedding。
长距离依赖问题： Self attention天然就能解决这个问题，因为在集成信息的时候，当前单词和句子中任意单词都发生了联系。
Transformer 的缺陷：
对于长输入的任务，典型的比如篇章级别的任务（例如文本摘要），因为任务的输入太长，Transformer会有巨大的计算复杂度，导致速度会急剧变慢。
Transformer整体结构确实显得复杂了一些，如何更深刻认识它的作用机理，然后进一步简化它，这也是一个好的探索方向。

三大特征抽取器比较
语义特征抽取能力： Transoformer 显著超越 RNN 和 CNN， RNN 和 CNN 差不多。
长距离特征捕获能力： CNN 显著弱于 RNN 和 Transformer。 Transformer微弱优于 RNN（主语谓语距离小于13），但在比较远的距离上（主语谓语距离大于13），RNN微弱优于Transformer。对于Transformer来说，Multi-head attention的head数量严重影响NLP任务中Long-range特征捕获能力：结论是head越多越有利于捕获long-range特征。
任务综合特征抽取能力： Transformer 强于 RNN 和 CNN
并行计算能力： Transformer 与 CNN 差不多，都远强于 RNN。
计算量： Transformer Block > CNN > RNN
训练速度： Transformer Base > CNN > Transformer Big > RNN
单从任务综合效果方面来说，Transformer明显优于CNN，CNN略微优于RNN。速度方面Transformer和CNN明显占优，RNN在这方面劣势非常明显。这两者再综合起来，如果我给的排序结果是Transformer>CNN>RNN.


2. 文本分类样本不平衡问题

  1. 对数据相对不足的类别上采样（oversampling the minority)
  2. 对数据相对过多的下采样（under-sampling the majority)
  3. SMOTE (Synthetic Minority Oversampling Technique) 对数据较少的类别人工采样
  4. 阈值调整（threshold moving），将原本默认为0.5的阈值调整到 较少类别/（较少类别+较多类别）即可 训练集是总体样本的无偏采样，观测几率就代表真实几率
  5. 对数据先进行聚类，再将大的簇进行随机欠采样或者小的簇进行数据生成
  6. 一分类

3. 文本分类多标签问题

已有的多标记学习算法的策略思路大致可以分为以下三类[[8]]：
  1. “一阶（first-order）”策略：该类策略通过逐一考察单个标记而忽略标记之间的相关性，如将多标记学习问题分解为个独立的二类分类问题，从而构造多标记学习系统。该类方法效率较高且实现简单，但由于其完全忽略标记之间可能存在的相关性，其系统的泛化性能往往较低。
  2. “二阶（second-order）”策略：该类策略通过考察两两标记之间的相关性，如相关标记与无关标记之间的排序关系，两两标记之间的交互关系等等，从而构造多标记学习系统。该类方法由于在一定程度上考察了标记之间的相关性，因此其系统泛化性能较优。
  3. “高阶（high-order）”策略：该类策略通过考察高阶的标记相关性，如处理任一标记对其它所有标记的影响，处理一组随机标记集合的相关性等等，从而构造多标记学习系统。该类方法虽然可以较好地反映真实世界问题的标记相关性，但其模型复杂度往往过高，难以处理大规模学习问题。

基本来讲，解决多标签分类问题有3种方法，分别是：
  1. 问题转换 二元关联、分类器链、LP法
  2. 自适应算法
  3. 集成方法


4. 基于知识库的对话系统的结构和工作原理？

5. 句子相似度的做法

句子相似度计算方法：
  1. 编辑距离计算
  2. 杰卡德系数计算
  3. TF 计算
  4. TFIDF 计算
  5. Word2Vec 计算

6. 文本分类有什么传统方法

常用分类算法的思路包括下面四种：
  1. 朴素贝叶斯分类器：利用特征项和类别的联合概率来估计文本的类别概率；
  2. 支持向量机分类器：在向量空间中找到一个决策平面，这个平面能够最好的切割两个分类的数据点，主要用于解决二分类问题；
  3. KNN 方法：在训练集中找到离它最近的 k 个临近文本，并根据这些文本的分类来给测试文档分类；
  4. 决策树方法：将文本处理过程看作是一个等级分层且分解完成的复杂任务。

7. word2vec的原理与改进方式；

https://www.cnblogs.com/pinard/p/7160330.html

  1. Hierarchical Softmax
  2. Negative Sampling


8. NLP基础任务，比如分词算法（序列标注任务），分类算法


### 机器学习

1. GBDT的原理

GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，又叫 MART（Multiple Additive Regression Tree)，它通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出。该算法将决策树与集成思想进行了有效的结合。

由于GBDT的核心在与累加所有树的结果作为最终结果，而分类树得到的离散分类结果对于预测分类并不是这么的容易叠加（稍等后面会看到，其实并不是简单的叠加，而是每一步每一棵树拟合的残差和选择分裂点评价方式都是经过公式推导得到的），而对基于回归树所得到的数值进行加减是有意义的（例如10岁+5岁-3岁=12岁），这是区别于分类树的一个显著特征（毕竟男+女=是男是女?，这样的运算是毫无道理的），GBDT在运行时就使用到了回归树的这个性质，它将累加所有树的结果作为最终结果。所以GBDT中的树都是回归树，而不是分类树，它用来做回归预测，当然回归树经过调整之后也能用来做分类。

梯度提升决策树GBDT是Boosting算法。
           基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。
算法的基本流程：在每一轮迭代中，首先计算出当前模型在所有样本上的负梯度，然后以该值为目标训练一个新的弱分类器进行拟合并计算出该弱分类器的权重，最终实现对模型的更新。

[机器学习算法系列（7）：GBDT](https://plushunter.github.io/2017/01/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%887%EF%BC%89%EF%BC%9AGBDT/)


2. 决策树节点分裂时如何选择特征，写出Gini index和information Gain的公式并举例说明

若利用一个特征进行分类的结果与随机分类的结果没有很大差异，则称这个特征是没有分类能力的。特征选择的准则是信息增益或信息增益比。直观上，若一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割为子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益可以表示这一直观的准则。

[机器学习算法系列（4）：决策树](https://plushunter.github.io/2017/01/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%884%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91/)

3. 分类树和回归树的区别是什么

分类树使用信息增益或增益比率来划分节点；每个节点样本的类别情况投票决定测试样本的类别。
回归树使用最大均方差划分节点；每个节点样本的均值作为测试样本的回归预测值。

4. 与Random Forest做比较，并以此介绍什么是模型的Bias和Variance

当我们谈论机器学习模型的误差的时候，这个误差可以主要分为两部分，bias和variance。一般情况下，模型需要在bias和variance之间取得一个平衡。bias小的模型，variance一般大；variance小的模型，bias一般大。更好的理解bias和variance的关系能够帮助我们更好的应付模型的过拟合和欠拟合问题。

Error due to Bias: Bias表示的就是模型预测的值和真实值之间的距离的期望。所以我们会通过建立多个模型（如使用不同的数据子集）来估计这个误差期望值。Bias代表着算法的拟合能力。

Error due to Variance: Variance表示的是当你对一个模型使用不同的数据进行多次建模时，这些模型在某一个点上的预测值的方差就是该模型在这个点上预测值的variance。其实就是预测值的方差的意思。Variance代表这算法的鲁棒性。

我们发现，通过对这n个模型进行平均，variance减小了，但bias没变。这就是bagging的功效，也就是为什么random forest更加鲁棒的原因（也可以用来解释神经网络中的dropout）。虽然bagging可以减小模型的方差，但是我们发现，并没有增加模型预测的准确率，也就是bagging减小了variance，却没减小bias。因而bagging中的模型是强模型，bias小，variance大。使用bagging，目标是降低variance。

为了减小模型的bias，我们可以使用boosting的方法（如adaBoost，GBDT，XGBoost)。Boosting中的模型是弱模型（比随机猜测好一点），bias大，variance小。如adaBoost在实现的时候常用决策树桩（也就是单层的决策树）。boosting的方法，会在之前模型预测误差即基础上继续建模预测，因而能够减小模型的bias，但由于此时模型之间不是独立的，所以variance的减小不显著。

[机器学习中的Bias和Variance](https://zhuanlan.zhihu.com/p/45213397)


5. XGBoost的参数调优有哪些经验

选择较高的学习速率(learning rate)。一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。选择对应于此学习速率的理想决策树数量。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。

对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。在确定一棵树的过程中，我们可以选择不同的参数，待会儿我会举例说明。

xgboost的正则化参数的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。

降低学习速率，确定理想参数。


6. XGBoost的正则化是如何实现的



7. XGBoost的并行化部分是如何实现的



8. 如果选用一种其他的模型替代XGBoost，会用什么



9. L1与L2的作用；



10. SVM的原理；

11. bagging与boosting方法的关系与区别

12. 控制过拟合的方式；

13. Xgboost相较boosting方式的优势，以及lightGBM相较Xgboost的提升

14. 朴素贝叶斯

15. 逻辑回归，线性回归

16. 决策树，不同的划分方式，ID3，C4.5，CTAR，XGBoost等等

17. Ensemble模型

18. SVM，核函数选择，不同SVM形式

19. HMM，CRF，如何轻松愉快地理解条件随机场（CRF）？

20. 最大熵原理，图解最大熵原理（The Maximum Entropy Principle）

21. KNN和K-Means，DBSACN也了解一下，以及各种距离计算方式，关于机器学习距离的理解


#### 深度学习

1. 手写损失函数

2. 介绍了一下attention机制，手写attention的公式，怎样计算得分和权重，soft attention和hard attention的区别

https://zhuanlan.zhihu.com/p/35739040

3. Seq2Seq的结构

4. fasttext、CNN、RNN的优缺点各是什么

5. 卷积神经网络相较于传统神经网络的优势；

6. 常用激活函数以及其变种；

7. LSTM、GRU是如何解决梯度消失问题的；

8. 深度神经网络是如何解决过拟合问题的；

9. 介绍下attention model的基本原理与应用场景；

10. 讲讲你对bert模型的理解；

11. CNN原理，如何用在文本上，在什么情况下适合用CNN，在什么情况下用LSTM

12. RNN系列，掌握RNN、LSTM和GRU的内部结构，RNN产生梯度消失的原因，LSTM如何解决，GRU对LSTM的改进

13. Word2vec工具，怎么训练词向量，skip-gram和cbow
https://zhuanlan.zhihu.com/p/35500923

14. 数据预处理，权重初始化，为什么不能全部初始化为0，词向量怎么预训练

15. 过拟合问题，原因是什么，怎么解决，主要从数据和模型两方面出发：机器学习中用来防止过拟合的方法有哪些？

16. 调参技巧，比如，卷积核大小怎么按层设置，bn放在哪里比较合适，激活函数之间的区别（sigmoid，tanh和relu），词向量维度怎么设置，等等。

17. 模型评估指标，acc，pre，recall，f1，roc曲线和auc曲线，分别适用于什么任务，怎么降低偏差，怎么降低方差，可以关注一下Hulu微信公众号：Hulu机器学习问题与解答系列 | 第一弹：模型评估

18. 优化方法，批量梯度下降，随机梯度下降，mini-batch梯度下降的区别，adam，adagrad，adadelta，牛顿法

19. 梯度消失问题，原因（链式求导，激活函数），解决方法（主要是batch norm）；以及梯度爆炸问题（梯度截断）

20. 关于训练集和验证集，为什么要划分，如何划分(留出法，交叉验证)

21. 如何处理数据不均衡问题，也是从数据和模型两方面出发解决。

22. BP后向传播过程的推导，可以参考：漫谈LSTM系列的梯度问题，先定义Loss函数，然后分别对输出层参数和隐藏层参数进行求导，得到参数的更新量。

21. softmax和交叉熵推导，分成i=j 和 ij 两种情况来算，参考这里：大师网-简单易懂的softmax交叉熵损失函数求导

22. 各种Loss函数

23. 似然函数，负对数似然函数的推导

23. 最小二乘法，利用矩阵的秩进行推导

24. 贝叶斯定理，拉普拉斯平滑

25. RNN在BP过程中梯度消失的原因，也把这个链式求导过程写出来。

26. 各种优化方法的公式，SGD，Momentum，Adagrad，Adam，机器学习优化方法总结比较 - 合唱团abc - 博客园

27. Batch Normalization，就是个归一化过程，再加一个scale操作

28. SVM推导，拉格朗日了解一下：机器学习之拉格朗日乘数法

29. 最大熵模型相关推导，一步一步理解最大熵模型 - wxquare - 博客园

30. pyTorch多线程

31. BERT

32. XLNet Transformer-XL

---
### 算法题

1. 算法题：股票买卖获最大利润

2. 复杂链表的复制，链表的删除

3. 最长公共子序列，逆序对

4. 快排，归并排序，堆排序

5. 二分查找，以及衍生的题目

6. 深度优先搜索
