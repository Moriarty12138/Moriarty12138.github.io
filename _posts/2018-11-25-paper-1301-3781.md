---
title: 【论文笔记】Efficient Estimation of Word Representations in Vector Space
date: 2018-11-25 11:10:31
---

[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)  


论文提出了两个新的模型，用来表示在大量数据集的情况下单词的连续向量。  
在单词相似性的方面来看，这两个模型的效果比之前的神经网络模型要好。而且在句法和语义相似上面也有很好的表现。  

---
许多当前的NLP系统和技术都将单词视为原子单位——单词之间没有相似性的概念，因为它们是用词汇表中的索引表示的。这种选择有几个很好的理由——简单性、健壮性以及基于大量数据的简单模型优于基于较少数据的复杂系统。一个例子是用于统计语言建模的流行的N-gram模型——今天，在几乎所有可用的数据(数万亿单词)上训练N-gram是可能的。  
然而，在许多任务中，这些简单的技术都处于极限。例如，用于自动语音识别的相关域内数据量是有限的——性能通常由高质量转录语音数据(通常只有数百万个单词)的大小决定。在机器翻译中，许多语言的现有语料库只包含几十亿个单词或更少。因此，在某些情况下，对基本技术进行简单的扩展不会导致任何重大进展，我们必须将重点放在更高级的技术上。  
随着近年来机器学习技术的进步，在更大的数据集上训练更复杂的模型已经成为可能，而且它们通常优于简单的模型。也许最成功的概念是使用单词[10]的分布式表示。例如，基于神经网络的语言模型显著优于N-gram模型  
本文的主要目的是介绍一些技术，这些技术可以用于从拥有数十亿词汇和数百万词汇的海量数据集中学习高质量的词汇向量。据我们所知，以前提出的架构都没有在更多的方面得到成功的培训超过几亿字，与维数适中的字向量之间50 - 100。  
我们使用最近提出的技术来测量结果向量表示的质量，期望相似的单词不仅彼此接近，而且单词可以有多个相似度[20]。这在前面的屈折语言中已经被观察到——例如，名词可以有多个词尾，如果我们在原始向量空间的一个子空间中搜索相似的词，就有可能找到具有相似词尾的词[13,14]。  
令人惊讶的是，研究发现，词汇表征的相似性超越了简单的句法规则。在对单词向量进行简单代数运算的情况下，使用单词偏移技术，例如向量(“国王”)-向量(“男人”)+向量(“女人”)得到的向量最接近单词Queen[20]的向量表示形式。  
在本文中，我们试图通过开发新的模型结构来最大限度地提高这些向量操作的准确性，以保持单词之间的线性规律。我们设计了一种新的综合测试集，用于同时测试句法和语义规则，结果表明，该测试集具有较高的学习精度。此外，我们还讨论了训练时间和精度是如何依赖于词向量的维数和训练数据量的。
单词作为连续向量的表示有着悠久的历史[10,26,8]。摘要在[1]中，提出了一种非常流行的神经网络语言模型(NNLM)的估计模型体系结构，利用线性投影层和非线性隐层的前馈神经网络联合学习词向量表示和统计语言模型。这项工作已被许多人仿效。  
NNLM的另一个有趣的架构是在[13,14]中提出的，在这个架构中，首先使用带有单个隐含层的神经网络来学习单词向量。然后使用vector这个词来训练NNLM。因此，即使没有构造完整的NNLM，也可以学习单词vector。在这项工作中，我们直接扩展了这个体系结构，并且只关注第一步，即使用一个简单的模型学习单词vector。  
后来的研究表明，向量这个词可以显著地改进和简化许多
NLP应用[4,5,29]。使用不同的模型体系结构对词向量本身进行估计，并在各种语料库上进行训练[4,29,23,19,9]，得到的一些词向量可用于未来的研究和比较2。然而，据我们所知，这些体系结构在训练上的计算成本明显高于[13]中提出的体系结构，除了使用对角权重矩阵[23]的对数-双线性模型的某些版本。  

---
提出了许多不同类型的词的连续表示估计模型，包括著名的潜在语义分析(LSA)和潜在狄利克雷分配(LDA)。
在本文中，我们将重点放在神经网络学习到的单词的分布式表示上，因为之前的研究表明，神经网络在保持单词之间的线性规律方面表现得明显优于LSA [20,31];此外，LDA在大数据集上的计算成本变得非常昂贵。  
与[18]类似，为了比较不同的模型架构，我们首先将模型的计算复杂性定义为需要访问的参数数量，以便对模型进行全面的培训。接下来，我们将尽量提高精度，同时最小化计算复杂度。
对于以下所有模型，训练复杂度与：  
O = E x T x Q  
其中E为训练时数，T为训练集中单词的个数，Q为每个模型架构的进一步定义。常见的选择是E = 3−50 T十亿。所有模型均采用随机梯度下降和反向传播[26]进行训练。  

---
在[1]中提出了概率前馈神经网络语言模型。它由输入、投影、隐藏和输出层组成。在输入层，前面N个单词使用1-of-V编码进行编码，其中V是词汇表的大小。输入层然后投射到投影层P维数N×D,使用共享的投影矩阵。由于在任何给定时间只有N个输入是活动的，投影层的合成是一个相对便宜的操作。    
由于投影层中的值比较密集，NNLM架构对于投影层和隐藏层之间的计算变得非常复杂。对于N = 10的常见选择，投影层(P)的大小可能是500到2000，而隐藏层的大小H通常是500到1000个单元。此外，隐藏层用于计算词汇表中所有单词的概率分布，从而得到一个维度为V的输出层。因此，每个训练示例的计算复杂度为  
Q = N x D + N x D x H + H x V   
控制项是H×V。但是，为了避免这种情况，提出了几个切实可行的解决办法;或者使用softmax的分层版本[25,23,18]，或者通过使用在培训期间没有标准化的模型来完全避免规范化模型[4,9]。使用词汇表的二叉树表示，需要计算的输出单元数可以减少到log2(V)左右。因此,大多数的复杂性是由D N××H。  
在我们的模型中，我们使用分层softmax，其中词汇表表示为Huffman二叉树。这与之前的观察结果一致，即单词的频率对于在神经网络语言模型[16]中获取类很有效。Huffman树将简短的二进制代码分配给频繁出现的单词，这进一步减少了需要计算的输出单元的数量:平衡二叉树需要计算log2(V)输出，而基于Huffman树的分层softmax只需要大约log2(Unigram perple(V))。例如，当词汇量为100万个单词时，这将使评估速度提高大约两倍。虽然这不是关键加速神经网络LMs的计算瓶颈D N××H,稍后我们将提出架构没有隐藏层,从而很大程度上取决于将softmax正常化的效率。  

 ---
 基于递归神经网络的语言模型被提出是为了克服前馈NNLM的某些局限性，如需要指定上下文长度(模型N的阶数)，以及由于理论上RNNs可以比浅层神经网络更有效地表示复杂的模式[15,2]。RNN模型没有投影层;只有输入、隐藏和输出层。这种模型的特殊之处在于使用延时连接将隐含层与自身连接起来的递归矩阵。这使得递归模型能够形成某种短期记忆，因为来自过去的信息可以通过根据当前输入和上一个时间步骤中隐藏层的状态进行更新的隐藏层状态来表示。  
 RNN模型每个训练实例的复杂度为：  
 Q = H x H + H x V  
 D表示这个词有相同的维度隐层H, H×V一词可以有效减少到H×log2通过使用分层softmax (V)。大部分的复杂性来自H×H。  

 ---
 为了在海量数据集上训练模型，我们在一个名为DistBelief[6]的大型分布式框架上实现了几个模型，包括前馈NNLM和本文提出的新模型。该框架允许我们并行运行同一模型的多个副本，每个副本通过保存所有参数的集中式服务器同步其梯度更新。对于这种并行训练，我们使用小型批处理异步梯度下降与自适应学习速率过程称为Adagrad[7]。在这个框架下，通常使用100个或更多的模型副本，每个模型副本使用数据中心中不同机器上的许多CPU核心。  

 ---
 在本节中，我们提出了两个新的模型体系结构，用于学习尽量减少计算复杂性的单词的分布式表示。上一节主要观察到，复杂性大部分是由模型中的非线性隐含层引起的。虽然这是神经网络如此吸引人的原因，但我们决定探索更简单的模型，这些模型可能无法像神经网络那样精确地表示数据，但可能能够有效地对更多数据进行训练。  
 新架构直接按照那些在我们之前提出的工作(13、14),在那里发现神经网络语言模型可以成功地训练两个步骤:第一,连续词向量是学会使用简单的模型,然后这些分布式的n元语法NNLM是训练上表示的单词。虽然后来有大量的工作专注于学习单词向量，但我们认为[13]中提出的方法是最简单的。注意，相关模型也在更早的时候被提出[26,8]。

 ---
第一个提出的架构类似于前馈NNLM，去掉非线性隐含层，对所有单词(不仅仅是投影矩阵)共享投影层;因此，所有单词都被投影到相同的位置(它们的向量被平均)。我们把这种结构称为一个词袋模型，因为历史上的词的顺序并不影响投影。此外，我们也使用来自未来的词汇;我们构建了一个log-linear classifier，在输入端有4个future和4个history单词，训练标准是正确的对当前(中间)单词进行分类，从而在接下来的部分介绍的任务中取得了最好的成绩。  
训练复杂度:  
Q = N x D + D x log2(V)  
我们将该模型进一步表示为CBOW，因为与标准的字袋模型不同，它使用了上下文的连续分布式表示。模型架构如图1所示。请注意，输入层和投影层之间的权重矩阵以与NNLM相同的方式共享所有单词位置。  

---
第二种架构类似于CBOW，但是它没有根据上下文预测当前单词，而是试图根据同一个句子中的另一个单词最大限度地对单词进行分类。
更准确地说，我们将每个当前单词作为连续投影层的对数线性分类器的输入，并在当前单词之前和之后的一定范围内预测单词。我们发现增加范围可以提高结果词向量的质量，但也会增加计算复杂度。由于距离较远的单词与当前单词的关系通常小于与当前单词的关系，因此在我们的训练示例中，通过从这些单词中抽取较少的样本，我们对距离较远的单词给予较少的权重。  
该体系结构的训练复杂度与:  
Q = C x (D + D x log2(V))  

其中C是单词的最大距离。因此，如果我们选择C = 5，对于每个训练词，我们将随机选择一个范围< 1的数字R;C >,从未来单词的当前单词作为正确的标签。这就要求我们做R×2词分类,与当前词作为输入,和每个R + R单词作为输出。在接下来的实验中，我们使用C = 10。     

---
为了比较不同版本的词向量的质量，以往的论文通常使用一个表来显示示例词及其最相似的词，并直观地理解它们。虽然很容易证明单词France与意大利(也许还有其他一些国家)类似，但是在更复杂的相似任务中对这些向量进行赋值时，会遇到更大的挑战，如下所示。我们遵循之前的观察，单词之间可以有很多不同类型的相似性，例如，单词big is similar to bigger在相同的意义上，small is similar to small。另一种关系类型的例子可以是单词对big - large和small - small[20]。我们进一步将两组关系相同的单词表示为一个问题，就像我们可以问的那样:“在同样的意义上，与小相似的单词与大相似的单词是什么?”  
