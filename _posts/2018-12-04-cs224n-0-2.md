---
title: 【NLP】【CS224N】lstm
date: 2018-12-04 13:48:31
---

### Recurrent Neural Networks

### The Problem of Long-Term Dependencies

One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame.  

One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame.

In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form.

Sadly, in practice, RNNs don't seem to be able to learn them.
The problem was explored in depth by [Hochreiter (1991) German](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf) and [Bengio](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf), et al.

Thankfully, LSTMs don’t have this problem!

### LSTM Networks

Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies.

They were introduced by [Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf) , and were refined and popularized by many people in following work.

LSTMs are explicitly designed to avoid the long-term dependency problem.Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!

The repeating module in a standard RNN contains a single layer.
![standard RNN contains](/images/DL-images/LSTM3-SimpleRNN.png)

The repeating module in an LSTM contains four interacting layers.
![LSTM contains](/images/DL-images/LSTM3-chain.png)

### The Core Idea Behind LSTMs

The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.

The cell state is kind of like a conveyor belt.It runs straight down the entire chain, with only some minor linear interactions.It's very easy for information to just flow along it unchanged.
![]()

---
参考资料：
1. [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
