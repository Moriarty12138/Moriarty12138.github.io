---
title: 【西瓜书】 005 神经网络
date: 2019-06-08 16:42:00
---

### 神经元模型

神经网络是由具有适应性的简单单元组成的广泛的并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的反应。

神经网络中最基本的成分是神经元模型，即上述定义中的“简单单元”。
在生物神经网络中，每个神经单元与其他神经元相连，当他“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；
如果某神经元的电位超过了一个“阈值”，那么它就会被激活，即“兴奋”起来，向其他神经元发送化学物质。
这就是“M-P神经元模型”。

![ml-05-1](/images/machine-learning/ml5.1.jpg)

在这个模型中，神经元接收到了来自n个其他神经单元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将于神经元的阈值进行比较，然后通过“激活函数”处理以产生神经元的输出。

理想的激活函数是阶跃函数，然而阶跃函数函数具有不连续、不光滑等不太好的性质，
因此常用sigmoid函数作为激活函数。
$$sigmoid(x) = \flac{1}{1 + e^{-x}}$$
又是也被称为“挤压函数”。

把许多这样的神经元按照一定的层次结构连接起来，就得到了神经网络。


### 感知机与多层网络

感知机由两层神经元组成，输入层接受外界信号后传递给输出层，输入层是M-P神经元，亦称“阈值逻辑神经元”。

感知机能容易的实现逻辑与、或、非运算。
$$y = f(\sum{w_i x_i} - \theta)$$

![ml-05-2](/images/machine-learning/ml5.2.jpeg)


给定训练数据集，权重$w_i (i = 1, 2, ...,n)$以及阈值$\theta$可通过学习得到。阈值$\theta$可看作一个固定输入为-1.0的“哑节点（dummy node）”所对应的连接权重$w_{n+1}$，这样权重和阈值的学习就可统一为权重的学习。

感知机的学习规则非常简单，对训练样例$(x, y)$，若当前感知机的输出为$\hat{a}$，则感知机的权重这样调整：

$$w_i <- w_i + \Delta w_i$$

$$\Delta w_i = \eta (y - \hat{y}) x_i$$

其中$\eta \in (0, 1)$称为学习率。
若感知机对训练样例预测正确，即$\hat{y} = y$，则感知机不发生变化，否则将根据错误的程度进行权重调整。

感知机只有输出神经元进行激活函数处理，即只拥有一层功能神经远，其学习能力非常有限。

若两类模式是线性可分的，即存在一个超平面能将他们分开，则感知机的学习过程一定会收敛而求得适当的权向量w；
否则感知机的学习过程将发生震荡，w难以稳定下来，不能求得合适解。


要解决非线性可分问题，需要考虑使用多层功能神经元。
输出层和输入层之间的一层神经元被称为隐层或者隐含层，隐含层和输出层神经元都是拥有激活函数的功能神经元。

每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络结构通常称为“多层前馈神经网络”。


神经网络的学习过程就是根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的阈值；
换言之，神经网络“学”到的东西蕴含在连接权与阈值中。


### 误差逆传递算法（BP算法）

给定数据集$D = {(x_1, y_1), (x_2, y_2), ...(x_m, y_m)}, x_i \in R^d, y_i \in R^l$，即输入示例由d个属性描述，输出l维实值向量。

![ml-05-3](/images/machine-learning/ml5.3.jpg)

![ml-05-4](/images/machine-learning/ml5.4.jpeg)

BP算法基于梯度下降（gradient descent）策略，以目标的负梯度方向对参数进行调整。

![ml-05-5](/images/machine-learning/ml5.5.jpeg)

BP算法的目标是要最小化训练集D上的累计误差
$$E = \frac{1}{m} \sum_{k=1}^{m}{E_k}$$

标准BP每次更新只针对单个样例，参数更新的非常频繁，而且对于不同样例进行更新效果可能出现抵消现象。
为了达到同样的累积误差最小化，标准BP算法往往需要多次迭代。

只需一个包含足够多神经元的隐层，多层前馈网络就能以任意精度逼近任意复杂的的连续函数。然而如何设置隐层神经元的个数仍是个未决问题，实际应用中通常靠“试错法”调整。


由于强大的表达能力，BP神经网络经常遭遇过拟合，其训练误差持续降低，但测试误差可能上升。
有两种策略来缓解BP网络的过拟合：
1. 早停：将数据分为训练集和验证集，训练集用来计算梯度、更新连接权和阈值，验证集用来估计误差。若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集的连接权和阈值。
2. 正则化：基本思想是在误差目标中增加一个用于描述网络复杂度的部分。误差目标函数变为：$E = \lambda \frac{1}{m} \sum_{k = 1}^{m}{E_k} + (1 - \lambda)\sum_{i}{w_i^2}$，其中$\lambda \in (0, 1)$用于对经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计。


### 全局最小与局部最小

若用$E$表示神经网络在训练集上的误差，则它显然是关于连接权$w$和阈值$\theta$的函数。
此时，神经网络的训练过程可看作一个参数寻优过程，即在参数空间中寻找一组最优参数使得$E$最小。

基于梯度的搜索是使用最广泛的参数寻优方法。
在此类方法中，我们从某些初始解出发，迭代寻找最优参数值。
每次迭代中，先计算误差函数在当前点的梯度，因此梯度下降法就是沿着负梯度方向搜索最优解。
若误差函数在当前点的梯度为零，则已达到局部最小值，更新量将为零。
这意味着，参数迭代更新将在此停止。
如果误差函数仅有一个局部最小，那么此时找到的就是全局最小，否则则不能保证是全局最小。


在现实任务中，通常采用一下策略来跳出局部最小，从而进一步接近全局最小：
1. 以多组不同的参数值初始化多个神经网络，按照标准方法训练后，取其中误差最小的解作为最终参数。这相当于从多个不同的初始点开始搜索，这样就可能陷入不同的局部最小，从中进行选择有可能活得更接近全局最小的结果。
2. 使用“模拟退火”技术。模拟退火在每一步都以一定的概率接受比当前解更差的结果，从而有助于“跳出”局部最小。在每次迭代过程中，接受“次优解”的概率要随着时间的推移而逐渐降低，从而保证算法稳定。
3. 使用随机梯度下降。与标准梯度下降法精确计算梯度不同，随机梯度下降法在计算梯度时加入了随机元素。于是，即使陷入了局部极小点，它计算出的梯度仍可能不为零，这样就有机会跳出局部极小继续搜索。
4. 遗传算法也常用来训练神经网络以更好的逼近全局最小。

上述跳出局部极小的技术大多是启发式，理论上尚缺乏保障。


### 其他常见神经网络

#### RBF网络

径向基（RBF）网络是一种单隐层前馈神经网络，它是使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合。

#### ART网络

竞争性学习是神经网络中的一种常用的无监督学习策略。
在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元被抑制。
这种机制亦称“胜者通吃”原则。（败者食尘！）

自适应谐振理论（ART）网络是竞争型学习的重要代表。
该网络由比较层、识别层、识别阈值和重制模块构成。
其中，比较层负责接收输入样本，并将其传递给识别层神经元。
识别层每个神经元对应一个模式类，神经元数目可在训练过程中动态增加以增加新的模式类。

#### SOM网络

自组织映射（SOM）网络是一种竞争学习型神经网络，它能将高维输入数据映射到低维空间，同时保持输入数据在高纬空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层的临近神经元。

#### 级联相关网络

一般的神经网络通常假定网络结构是事先固定的，训练的目的是利用训练样本来确定合适的连接权、阈值等参数。
结构自适应网络将网络结构也当作学习目标之一，并希望能在训练过程中找到最符合数据特点的网络结构。
级联相关网络是结构自适应网络的重要代表。

#### Elman网络

与前馈神经网络不同，“递归神经网络”允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号。

#### Boltzmann机

神经网络中有一类模型是为网络状态定义一个“能量”（energy），
能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数。
Boltzmann机就是一种“基于能量的模型”。
其神经元分为两层：显层和隐层。
显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达。

### 深度学习

理论上来说，参数越多的模型复杂度越高、容量越大，这意味着它能完成更复杂的学习任务。

典型的深度学习模型就是很深层的神经网络。
显然，对神经网络模型，提高容量的一个简单方法就是增加隐层的数目。
从增加模型复杂度的角度来看，增加隐层的数目显然比增加隐层神经元的数目更加有效。


无监督逐层训练是多隐层网络训练的有效手段，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为下一层隐节点的输入，这称为“预训练”（pre-training）；
在训练全部完成后，再对整个网络进行“微调”（fine-tuning）训练。

事实上，“预训练+微调”的做法可视为将大量参数分组，对每组先找到局部看来比较好的设置，然后在基于这些局部较优的结果联合起来进行全局寻优。
这样就在利用了模型大量参数所提供的自由度的同事有效的节省了训练开销。

另一个计生训练开销的策略是“权共享”，即让一组神经元使用相同的连接权。
这个策略在卷积神经网络中发挥了重要作用。


从另一个角度来理解深度学习，无论是DBN还是CNN，其多隐层堆叠、每层对上一层的输出进行处理的机制，可看作对输入信号进行逐层加工，，从而把初始的、与输出目标联系更密切的表示，使得原来仅基于最后一层的输出映射难以完成的任务成为可能。
换言之，通过多层处理，逐渐将初始的“低层”特征表示转换为“高层”特征表示，用“简单模型”即可完成复杂的分类等学习任务。
由此可将深度学习理解为进行“特征学习”（feature learning）或“表示学习”（representation learning）。


以往在机器学习用于现实任务时，描述样本的特征通常需由人类专家设计，这称为“特征工程”（feature engineering）。
特征学习则通过机器学习技术自身来产生好特征，这使机器学习向“全自动数据分析”又前进了一步。
