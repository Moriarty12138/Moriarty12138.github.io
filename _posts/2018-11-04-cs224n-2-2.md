---
title: 【NLP】【CS224N】2.2 word2vec
date: 2018-11-04 14:48:31
---

由于之前的视频看的很仓促，有很多没有总结到。甚至于后面的部分完全没看懂，再重新看一遍。  
一来是语言，即使加上中文字幕还是学的很勉强。    
二来前面学的很仓促，需要补充的有点多。  
所以整个进度会很慢。  
这也是没办法的事。  

---
学习计划：
1. word meaning  词义
2. word2vec introduction  介绍word2vec词汇向量模型
3. research highlight  
4. word2vec objective function gradients
5. optimization refresher
6. assignment 1 notes
7. usefulness of word2vec

---
### 如何表示词义
目前最常用的方法：用分类资源来处理词义。  
对于英语而言，最有名的分类资源就是WordNet。  

存在着大量的同义词资源，但是很难从这些资源中尽可能多的获取价值。  
细微的差别很难区分。  
一些比喻性的表达，wordnet中找不到。  
很难对词义的相似性给出准确的定义。  
--离散化表征或分类学表征普遍存在的问题  

几乎所有的NLP研究，除了现代深度学习，以及80年代做的一点NLP神经网络以外，其他所有的NLP都使用了原子符号来表示单词。  
如果从神经网络的角度来考虑这个问题，那么使用原子符号就像使用只有一个位置是1，其他全是0的大向量。  
这样就有了大量与原子符号相对应的词汇。  
而这样的向量会非常长。  

-->神经网络的独热码(one-hot)  
独热词汇向量是一种存储在某处的本地表示。  


分布相似性  (distributional similarity)
只需观察一个词出现的上下文，就可以找大量词来表示这个词的含义。  

为每个单词建立一个密集向量，让他可以预测目标单词所在文本的其他词汇。  


### 什么是word2vec  
一种解决学习神经词嵌入问题的通用方法。  
定义一个模型，根据中心词汇预测它上下文的词汇。  
用一些概率方法根据给定单词预测上下文单词出现的概率。  
再用损失函数来判断预测的准确性。  
